---
title: "Project2"
author: "Christian Lehre, Erik Bøe & Axel Rønold"
date: "3/5/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

# Problem 1
```{r,eval = TRUE,echo = FALSE}
ds <-  read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/diabetes.csv",sep = ",")
#apply(ds,2,summary)
#pairs(ds,pch = '.')
library(ggplot2)
full <- lm(prog ~.,data = ds)
summary(full)
plot(full$fitted, rstudent(full), pch = 20)
ggplot(full,aes(full$fitted,rstudent(full))) + geom_point(pch= 21) + geom_hline(yintercept = 0,linetype = 'dashed') + geom_smooth(se = FALSE,col = 'red',size = 0.5,method = 'loess')
#plot(lm(rstudent(full)~full$fitted))
qqnorm(rstudent(full),pch = 20)
qqline(rstudent(full),col = 2)
library(nortest)
ad.test(rstudent(full))


```
**a)**

**1.**

* `Estimate`
\newline 
Estimated regression coefficients given by 
$$\hat{\boldsymbol{\beta}} = (X^TX)^{-1}X^T\mathbf{y},$$
\newline where $X$ is the design matrix of the regression, $\mathbf{y}$ is a vector containing the observed response values and $\hat{\boldsymbol{\beta}}$ is a vector containing the estimated regression coefficients. The interpretation of the regression coefficients is that if we increase covariate $x_j$ with one unit, and keeping all other covariates constant, the response variable changes with a factor of $\hat{\beta_j}$.

* `Std.Error`
\newline 
Estimated standard deviation of the estimated regression coefficients, i.e the average amount that the estimated regression coefficients vary from the actual value. The Std.Error is given by 
$$\widehat{\text{SD}}(\hat{\beta_j}) = \sqrt{\hat{\sigma}^2(X^TX)^{-1}_{jj}},$$ where $X$ is the design matrix of the regression, and $\hat{\sigma}$ is the residual standard error.

* `t value`
\newline
Value of the t-statistic for testing whether the corresponding regression coefficient is different from 0. The t-value is given by 
$$t = \frac{\beta_j-\hat{\beta_j}}{\widehat{\text{SD}(\hat{\beta_j})}},$$ where $\beta_j$ is the regression coefficient for covariate $x_j$, $\hat{\beta_j}$ the estimated regression coefficient and $\widehat{\text{SD}(\hat{\beta_j})}$ the Std.Error as explained above.

* `P(>|t|)`
\newline
Probability of observing a more extreme value than $t$ under the null hypothesis, i.e the so-called p-value used in hypothesis testing. The p-value is given by
$$
2P(T < |t| : H_0)
$$
Assuming a two-sided hypothesis test. $H_0$ is the null-hypothesis, $T$ is a random vector and $t$ is the observed value

**2.**
\newline
The estimate for the intercept can be interpreted as the value of the response when all covariates are set to zero. This is not realistic espescially for models when some of the covariates cannot simply be zero, e.g Age and Weight or binomials which does not have zero in its domain. 

**3.**
\newline
The estimated regression coefficient for BMI can be interpreted as the change in the response variable when the BMI increases by one. In our example it is given as 5.6, which means that if a person increase his or her BMI by 1, the estimate for the progression of the disease after one year will increase by 5.6.

**4.**
\newline
The estimated error standard deviation is found in the summary as the Residual standard error, and is interpreted as the mean deviance between the estimated and true regression line. The estimated error variance is found by squaring the Residual standard error. 
The formula for the estimated error variance is 
$$\hat{\sigma}^2 = \frac{RSS}{n-p} = \frac{\sum_{i = 1}^n(y_i - \hat{y_i})^2}{n-p}$$
where n is the number of observations and p is the number of covariates in the model.

**5.**
\newline
The covariates that are significant at level 0.05 are the ones with a p-value below this. From the printed model summary we see that the covariates sex, bmi, map and ltg are significant at level 0.05. The null hypothesis related to this p-value is that the covariates are equal to zero, while the alternative hypothesis is that they are unequal to zero,
\begin{equation}
     H_0: \beta_j=0, \qquad H_1: \beta \neq 0.
\end{equation}
For the p-value to be valid, the assumption of independently and identically normally distributed errors, i.e., $\varepsilon \sim N(0,\sigma^2)$, is needed.

**b) Model Evaluation**

The Anderson-Darling normality test yields a high p-value, and thus the hypothesis that the residuals are normally distributed is accepted. 
However, one observes from figure 2 that the studentized residuals are heteroscedatic, and thus the errors are heteroscedatic. This is problematic, as the linear model assumes homoscedatic errors.
There is no clear evidence of any non-linear effects of the covariates looking at the studentized residuals.However, there seems to be a tendency of the residuals lying above zeros in the left of the plot and then more below zero as we move to the right, which could be a sign of autocorrelated errors.

Looking at the scatter-plot between all the covariated in figure 2, it is clear that some of the covariates are highly correlated, e.g tc and ldl. This results in imprecise estimation of the regression parameters, as the variance is high in this situation.

To look into the significance of the regression one can use the F-test: 
$$
H_0: \beta_j = 0 \;\;\forall j = 0, \dots ,p \quad H_1: \text{at least one} \; \beta_j \ne 0
$$
For this model the F-test yields a low p-value, and the null-hypothesis is rejected. Thus, the regression is significant.

The $\textbf{Multiple R-squared}$ in figure 1 is the coefficient of determination defined by 
\begin{equation}
R^2=\frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n ({y_i}-\bar{y})^2}=1-\frac{\hat{\varepsilon}_i^2}{\sum_{i=1}^n ({y_i}-\bar{y})^2},
\end{equation}
which is a measurement of the amount of variance that is explained by the model. We observe that the closer $R^2$ is to 1, the smaller the residual
sum of squares $\varepsilon_i^2$ are, and thus the better the fit to the data. 

In terms of proportion of variance explained by this model, the fit is pretty poor. Both $R^2$ and adjusted $R^2$ yields a pretty low value of roughly 51% explained variance.

**c)**
```{r,eval = FALSE,echo = FALSE}
library(leaps)
allsubs <- regsubsets(prog ~.,data = ds, nvmax = 10)
allsummary <- summary(allsubs)
allsummary$outmat
which.max(allsummary$adjr2)
plot(allsummary$adjr2,xlab = 'Variables',ylab = 'R2adj',type = 'l')
plot(allsubs,scale = 'adjr2')
which.min(allsummary$bic)
plot(allsummary$bic,xlab = 'variables',ylab = 'BIC',type = 'l')
plot(allsubs,scale = 'bic')
```
The more covariates included in a model, the more complex the model is, and the model becomes less biased. Thus, by the biad-variance tradeoff, the more complex a model is, the higher the variance of the resulting model. When doing predictions, one obtains a more accurate prediction if the variance is low. Hence, a reduced model results in lower variance, and might perform better when doing predictions. 

As for the best subset model selection method, one fit for every covariate $k = 0,...,p$, all the $p\choose k$ models containing k covariates, and pick the best model (which contains an equal amount of covariates) in terms of the $R^2$-metric. Once the methods has found the best $p$ models each containing a different amount of covariates, the method choose the ultimately best model in terms of some model choice criteria, e.g BIC or $R^2_{\text{adj}}$.

BIC is an abbreviation of Bayesian Information Criterion, and is defined as 
$$
\text{BIC} = \frac{RSS/n}{\hat{\sigma}^2} + \log{n}\frac{p}{n} = \frac{\sum_{i = 1}^n(y_i - \hat{y_i})^2/n}{\hat{\sigma}^2} + \log{n}\frac{p}{n}
$$
The model with the lowest BIC is the preferred model.

$R^2_{\text{adj}}$ is a measure of proportion of variability explained by a model, where the number of covaraites included in the model is taken into account. $R^2_{\text{adj}}$ is defined as

$$
R^2_{\text{adj}} = 1 - \frac{RSS/(n-p)}{TSS/(n-1)} = 1  -\frac{\sum_{i = 1}^n(y_i-\hat{y_i})^2/(n-p)}{\sum_{j = 1}^n(y_i-\bar{y})^2/(n-1)}
$$
Note that $R^2_{\text{adj}}$ includes a penalty on the number of included covariates $p$ in the model, and that $p = 1$ corresponds the the multiple $R^2$.
The model with highest $R^2_{\text{adj}}$ is the preferred model.

The 10 different models was calculated using the regsubsets-command in R, which use the "leaps and bounds" algorithm. The algorithms return the optimal model, and thereby avoid the computation of all $2^p$ models, where $p$ is the number of covariates in the full model. 

Based on BIC, the best reduced model contains 5 covariates, while the best reduced model based on $R^2_{\text{adj}}$ contains 8. The best model for each model selection criteria is found in the best subset selection matrix above. THe resulting model fits are shown in the code-chunk below. 

One observes that all covariates are significant in the less complex model, i.e the model chosen by the BIC criteria. The model chosen by the $R^2_{\text{adj}}$ criterion contains unsignificant variables as well. To choose the best of these two models we also look at how BIC and $R^2_{\text{adj}}$ behaves with varying included covariates. If one look at the plot of $R^2_{\text{adj}}$ one observes that the $R^2_{\text{adj}}$ is pretty much stable in the interval 5-10 covariates. However, in the BIC-plot, one observes a low at 5 covariates followed by a clear increase up to the full model. Therefore 5 covariates is closest to optimal when considering both criterions. 
Based on this, we chose to go forward with the model selected by the BIC criterion, i.e the model with 5 covariates. This model will from now on be referred to as the reduced model.The reduced fitted regression model is given by,
$$
\widehat{\text{Prog}} = -240.0 - 22.4\text{Sex} + 5.6\text{Bmi} + 1.1\text{Map} -1.1\text{Hdl} + 99.5\text{Ltg} 
$$

```{r,eval = TRUE,echo = TRUE}
bic.fit = lm(prog~sex+bmi+map+hdl+ltg,data = ds)
summary(bic.fit)

r2adj.fit = lm(prog~sex+bmi+map+tc+ldl+tch+ltg+glu,data = ds)
summary(r2adj.fit)
```
```{r,eval = TRUE}
model.full = summary(full)
model.full
```
As mentioned above, all included covariates in the reduced model are significant. The full model, however, contains unsignificant covariates. Including these irrelevant variables causes the precision of the estimators to decrease due to overfitting, and thus we prefer the reduced model.

The overall standard error is less in the reduced model than the full, meaning that the prediction from the reduced model is more accurate.

**d)**

```{r,eval = TRUE}
p_age = model.full$coefficients['age','Pr(>|t|)']
p_tc =  model.full$coefficients['tc','Pr(>|t|)']
p_ldl =  model.full$coefficients['ldl','Pr(>|t|)']
p_tch =  model.full$coefficients['tch','Pr(>|t|)']
p_glu =  model.full$coefficients['glu','Pr(>|t|)']
p_tot = p_age*p_tc*p_ldl*p_tch*p_glu
p_tot

#model.test = lm(prog~age+tc+ldl+tch+glu,data = ds)
#print(anova(full,model.test))
```

# Problem 2

**a)**
```{r,eval = TRUE}
pvalues <-scan("https://www.math.ntnu.no/emner/TMA4267/2018v/pvalues.txt")

alpha = 0.05
rejected = 0
for (i in pvalues){
  if (i < alpha){
    rejected = rejected +1
  } 
}
rejected
```
Based on our data, and a significance level of 0.05, we reject 155 of 1000 null-hypotheses.
A false positive finding, i.e a type 1 error, is the case when one reject a true $H_0$
With a significance level of 0.05, we say that the probability of making a type 1 error is 0.05. 

We can not say how many of these rejected null-hypothesis that were actually true, as our data only consists of the p-values. 
However, we could try to estimate it using the number of rejected null-hypothesis and the significance level, yielding $155\cdot0.05 =$ `r rejected*alpha` type 1 errors on average.

**b) **
The Familiwise Error Rate (FWER) is the probability of making at least one type 1 error in a series of hypothesis tests. The term family comes from the family of tests, which is the technical term for a series of tests on data.

To control the FWER at level 0.05, one would adjust the threshold for where the rejection of the null-hypothesis is made such that the probability of making at least one type 1 error is stable on 0.05.

The cutoff for p-values when using the Bonferroni method should be set to 0.05 divided by the number of null-hypothesis, so in our case this number would yield $0.05/1000 = 5*10^{-5}$.

```{r,eval = TRUE}
pvalues <-scan("https://www.math.ntnu.no/emner/TMA4267/2018v/pvalues.txt")

alpha = 0.05/1000
rejected = 0
for (i in pvalues){
  if (i < alpha){
    rejected = rejected +1
  } 
}
rejected
```
For our p-values, the number of rejections with the Bonferroni method is 50.

**c)**

Assuming that the first 900 null-hypotheses true, and last 100 false. We first investigate the number of type 1 and type 2 errors with $\alpha = 0.05$ as in problem a)
```{r,eval = TRUE}

errors<-function(alpha){
  typ1 = 0
  typ2 = 0
  for (i in seq(from = 1, to = length(pvalues))){
    if (i <= 900){
      if (pvalues[i] < alpha){
        typ1 = typ1 +1
      }
    }
  
    else{
      if (pvalues[i] >= alpha){
        typ2 = typ2 +1
      }
    }
  }
  return (c(typ1,typ2))
}

alpha_a = 0.05
alpha_b = 0.05/length(pvalues)

error_a = errors(alpha_a)
error_b = errors(alpha_b)
print("Type 1 error a):")
print(error_a[1])
print("Type 2 error a):")
print(error_a[2])

print("Type 1 error b):")
print(error_b[1])
print("Type 2 error b):")
print(error_b[2])
```
With $\alpha = 0.05$ as in problem a), we observe 55 type 1 errors and 0 type 2 errors. Which indicates that we could probably lower the significance level in this situation. However, it depends on how important it is to avoid type 2 errors compared to avoiding type 1 errors.

For $\alpha = 5\cdot 10^{-5}$ as in problem b) the same code yields,

With $\alpha = 5\cdot 10^{-5}$ as in problem b), we observe 0 type 1 errors and 50 type 2 errors. Which indicates that we could probably increase the significance level in this situation. However, it depends on how important it is to avoid type 1 errors compared to avoiding type 2 errors.