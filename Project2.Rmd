---
title: "Project2"
author: "Christian Lehre, Erik Bøe & Axel Rønold"
date: "3/5/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
```

# Problem 1
```{r,eval = TRUE,echo = FALSE}
ds <-  read.csv("https://web.stanford.edu/~hastie/CASI_files/DATA/diabetes.csv",sep = ",")
#apply(ds,2,summary)
#pairs(ds,pch = '.')
full <- lm(prog ~.,data = ds)
#summary(full)
#plot(full$fitted, rstudent(full), pch = 20)
#qqnorm(rstudent(full),pch = 20)
#qqline(rstudent(full),col = 2)
#library(nortest)
#ad.test(rstudent(full))
```
**a)**


**1.**
\newline
* `Estimate` - in particular interpretation of `Intercept`
\newline 
Estimated regression coefficients given by 
$$\hat{\boldsymbol{\beta}} = (X^TX)^{-1}X^T\mathbf{y}$$.
\newline where $X$ is the design matrix of the regression, $\mathbf{y}$ is a vector containing the observed response values and $\hat{\boldsymbol{\beta}}$ is a vector containing the estimated regression coefficients. The interpretation of the regression coefficients is that if we increase covariate $x_j$ with one unit, and keeping all other covariates constant, the response variable changes with a factor of $\hat{\beta_j}$.

* `Std.Error`
\newline 
Estimated standard deviation of the estimated regression coefficients, i.e the average amount that the estimated regression coefficients vary from the actual value. The Std.Error is given by 
$$\widehat{\text{SD}}(\hat{\beta_j}) = \sqrt{\hat{\sigma}^2(X^TX)^{-1}_{jj}}$$, where $X$ is the design matrix of the regression, and $\hat{\sigma}$ is the residual standard error.

* `t value`
\newline
Value of the t-statistic for testing whether the corresponding regression coefficient is different from 0. The t-value is given by 
$$t = \frac{\beta_j-\hat{\beta_j}}{\widehat{\text{SD}(\hat{\beta_j})}}$$, where $\beta_j$ is the regression coefficient for covariate $x_j$, $\hat{\beta_j}$ the estimated regression coefficient and $\widehat{\text{SD}(\hat{\beta_j})}$ the Std.Error as explained above.

* `P(>|t|)`
\newline
Probability of observing a more extreme value than $t$ under the null hypothesis, i.e the so-called p-value used in hypothesis testing. The p-value is given by
$$
2P(T < |t| : H_0)
$$
Assuming a two-sided hypothesis test. $H_0$ is the null-hypothesis, $T$ is a random vector and $t$ is the observed value

**2.**
\newline
The estimate for the intercept can be interpreted as the value of the response when all covariates are set to zero. This is not realistic espescially for models when some of the covariates cannot simply be zero, e.g Age and Weight or binomials with values not equal to zero. 

**3.**
\newline
The estimated regression coefficient for BMI can be interpreted as the change in the response variable when the BMI increases by one. In our example it is given as 5.6, which means that if a person increase his or her BMI by 1, the estimate for the progression of the disease after one year will increase by 5.6.

**4.**
\newline
The estimated error standard deviation is found in the summary as the Residual standard error, and is interpreted as the mean deviance between the estimated and true regression line. The estimated error variance is found by squaring the Residual standard error. 
The formula for the estimated error variance is 
$$\hat{\sigma}^2 = \frac{RSS}{n-p} = \frac{\sum_{i = 1}^n(y_i - \hat{y_i})^2}{n-p}$$
where n is the number of observations and p is the number of covariates in the model.

**5.**
\newline
The covariates that are significant at level 0.05 are the ones with a p-value below this. From the printed model summary we see that the covariates sex, bmi, map and ltg are significant at level 0.05. The null hypothesis related to this p-value is that the covariates are equal to zero, while the alternative hypothesis is that they are unequal to zero,
\begin{equation}
     H_0: \beta_j=0, \qquad H_1: \beta \neq 0.
\end{equation}
For the p-value to be valid, the assumption of independently and identically normally distributed errors, i.e., $\varepsilon \sim N(0,\sigma^2)$, is needed.

**b) Model Evaluation**

The Anderson-Darling normality test yields a high p-value, and thus the hypothesis that the residuals are normally distributed is accepted. 
However, one observes from figure 2 that the studentized residuals are heteroscedatic, and thus the errors are heteroscedatic. This is problematic, as the linear model assumes homoscedatic errors.
There is also no evidence of any non-linear effects of the covariates looking at the studentized residuals. Or does the residuals lie a bit over the zero mean in the beginning and below in the end(??).

To look into the significance of the regression one can use the F-test: 
$$
H_0: \beta_j = 0 \;\;\forall j = 0, \dots ,p \quad H_1: \text{at least one} \; \beta_j \ne 0
$$
For this model the F-test yields a low p-value, and the null-hypothesis is rejected. Thus, the regression is significant.

The $\textbf{Multiple R-squared}$ in figure 1 is the coefficient of determination defined by 
\begin{equation}
R^2=\frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n ({y_i}-\bar{y})^2}=1-\frac{\hat{\varepsilon}_i^2}{\sum_{i=1}^n ({y_i}-\bar{y})^2},
\end{equation}
which is a measurement of the amount of variance that is explained by the model. We observe that the closer $R^2$ is to 1, the smaller the residual
sum of squares $\varepsilon_i^2$ are, and thus the better the fit to the data. 

In terms of proportion of variance explained by this model, the fit is pretty poor. Both $R^2$ and adjusted $R^2$ yields a pretty low value of roughly 51% explained variance.

**c)**
```{r,eval = FALSE,echo = FALSE}
library(leaps)
allsubs <- regsubsets(prog ~.,data = ds, nvmax = 10)
allsummary <- summary(allsubs)
allsummary$outmat
which.max(allsummary$adjr2)
plot(allsummary$adjr2,xlab = 'Variables',ylab = 'R2adj',type = 'l')
plot(allsubs,scale = 'adjr2')
which.min(allsummary$bic)
plot(allsummary$bic,xlab = 'variables',ylab = 'BIC',type = 'l')
plot(allsubs,scale = 'bic')
```
The more covariates included in a model, the more complex the model is, and the model becomes less biased. Thus, by the biad-variance tradeoff, the more complex a model is, the higher the variance of the resulting model. When doing predictions, one obtains a more accurate prediction if the variance is low. Hence, a reduced model results in lower variance, and might perform better when doing predictions. 

As for the best subset model selection method, one fit for every covariate $k = 0,...,p$, all the $p\choose k$ models containing k covariates, and pick the best model (which contains an equal amount of covariates) in terms of the $R^2$-metric. Once the methods has found the best $p$ models each containing a different amount of covariates, the method choose the ultimately best model in terms of some model choice criteria, e.g BIC or $R^2_{\text{adj}}$.

BIC is an abbreviation of Bayesian Information Criterion, and is defined as 
$$
\text{BIC} = \frac{RSS/n}{\hat{\sigma}^2} + \log{n}\frac{p}{n} = \frac{\sum_{i = 1}^n(y_i - \hat{y_i})^2/n}{\hat{\sigma}^2} + \log{n}\frac{p}{n}
$$
The model with the lowest BIC is the preferred model.

$R^2_{\text{adj}}$ is a measure of proportion of variability explained by a model, where the number of covaraites included in the model is taken into account. $R^2_{\text{adj}}$ is defined as

$$
R^2_{\text{adj}} = 1 - \frac{RSS/(n-p)}{TSS/(n-1)} = 1  -\frac{\sum_{i = 1}^n(y_i-\hat{y_i})^2/(n-p)}{\sum_{j = 1}^n(y_i-\bar{y})^2/(n-1)}
$$
Note that $R^2_{\text{adj}}$ includes a penalty on the number of included covariates $p$ in the model, and that $p = 1$ corresponds the the multiple $R^2$.
The model with highest $R^2_{\text{adj}}$ is the preferred model.

The 10 different models was calculated using the regsubsets-command in R, which use the "leaps and bounds" algorithm. The algorithms return the optimal model, and thereby avoid the computation of all $2^p$ models, where $p$ is the number of covariates in the full model. 


Based on BIC, the best reduced model contains 5 covariates, while the best reduced model based on $R^2_{\text{adj}}$ contains 8. The best model for each model selection criteria is found in the best subset selection matrix above. THe resulting model fits are shown in the code-chunk below. 

Comparing these two model fits by means of $R^2_{\text{adj}}$, one observes that the best model results from the $R^2_{\text{adj}}$-model selection criteria. This will from now on be referred to as the reduced model. The fitted regression model for the reduced model is 

$$
\widehat{\text{Prog}} = -328.6 - 23.0\text{Sex} + 5.6\text{Bmi} + 1.1\text{Map} -0.8\text{Tc} + 0.5\text{Ldl} + 4.7\text{Tch} + 144.9\text{Ltg} + 0.3\text{Glu}
$$

```{r,eval = TRUE,echo = TRUE}
bic.fit = lm(prog~sex+bmi+map+hdl+ltg,data = ds)
summary(bic.fit)
r2adj.fit = lm(prog~sex+bmi+map+tc+ldl+tch+ltg+glu,data = ds)
summary(r2adj.fit)
```


```{r,eval = TRUE}
model.full = summary(full)
model.full
```

```{r,eval = TRUE}
model.reduced = summary(r2adj.fit)
model.reduced
```
Based on the $R^2_{\text{adj}}$-metric, the reduced model performs better, i.e the reduced model explains a greater proportion of variability in the data. However, the difference is not great ($\sim 0.002$).

Both models contains an equal amount of significant covariates, but the covariate tc is found to be significant at a lower level of significance. 

The overall standard error is less in the reduced model than the full, meaning that the prediction from the reduced model is more accurate.

The estimated regression parameters is lower in the reduced model, i.e the contribution to the response from a unit increase of a single covariate, while all others are kept constant, are lower in the reduced model. 
